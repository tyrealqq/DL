英文版
P85 本部分解释局部最优及全局最优的概念，常用的方法为梯度下降的方法。
结合P91 的图4.6，可以看到其思想是随机梯度下降的方法，可以一定程度解决局部最优到全局最优的问题。
参看https://www.zybuluo.com/hanbingtao/note/448086 的随机梯度下降的图解。(Stochastic Gradient Descent, SGD)

如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是SGD算法。在SGD算法中，每次更新的迭代，只计算一个样本。
这样对于一个具有数百万样本的训练数据，完成一次遍历就会对更新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新并不一定按照减少的方向。
然而，虽然存在一定随机性，大量的更新总体上沿着减少的方向前进的，因此最后也能收敛到最小值附近。
SGD不仅仅效率高，而且随机性有时候反而是好事。今天的目标函数是一个『凸函数』，沿着梯度反方向就能找到全局唯一的最小值。
然而对于非凸函数来说，存在许多局部最小值。随机性有助于我们逃离某些很糟糕的局部最小值，从而获得一个更好的模型。

P100：机器学习的几类问题：
1. 分类问题。输出对应各分类的概率的向量。也包含缺失输入的分类问题。e.g. 医疗疾病分类，病灶问题。
2. 回归问题。和分类问题类似，但是输出为数值。e.g. 保额问题
3. Transcription, 对非结构化数据的理解，e.g. 看图总结内容
4. Machine Translation， 输入符号序列并转化输出为另一符号序列，翻译
5. 结构化输出问题，我的理解是几种问题的集合体。
6. Anormaly detection，学习习惯、偏好后，对异常行为的识别，e.g. 信用卡对异常支付行为的识别。
7. Synthesis and sampling, 生成与学习的训练集相似的结构化数据
8. 缺失值的生成
9. 降噪
10. 密度及概率函数的估计

几组概念
1. 监督学习（标签）和无监督学习（聚类）
2. 过拟合与欠拟合
